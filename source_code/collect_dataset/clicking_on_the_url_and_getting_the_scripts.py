# -*- coding: utf-8 -*-
"""clicking on the url and getting the scripts .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16WDGOsEsM_y6GdYPq6qjunQCS2zL_ohS
"""


import csv
import pandas as pd
import math
import time
import random
import math
import urllib.request
import json
import pandas as pd
from pathlib import Path
import os
import requests
import urllib.request as urllib2
from pprint import pprint


def get_tokens():
    tokens = ["123456789"] #add your token as a string to this list. 
    return tokens

class GitHub:
    def __init__(self, url, ct):
        self.ct = ct
        self.url = url
    def getResponse(self):
        jsonData = None
        request_code=200
        try:
            if self.ct == len(get_tokens()):
                self.ct = 0
            reqr = urllib.request.Request(self.url)
            reqr.add_header('Authorization', 'token %s' % get_tokens()[self.ct])
            opener = urllib.request.build_opener(urllib.request.HTTPHandler(debuglevel=1))
            request_code=opener.open(reqr).getcode()
            content = opener.open(reqr).read()
            self.ct += 1
            jsonData = json.loads(content)
        except Exception as e:
            if 'HTTP Error 404' in str(e):
              request_code=404
            elif 'HTTP Error 401' in str(e):
              request_code=401
            elif not 'HTTP Error 403' in str(e):
                request_code=404
            
            print("e is:",e, self.url)
        return jsonData, self.ct,request_code
    
version = 0
path_output ='../data/'
download_path='../data/downloaded_code/'
data_content_file = open(path_output + 'url_of_download_content_{}.csv'.format(version), mode='w', newline='',encoding='utf-8')
data_content_writer = csv.writer(data_content_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
data_content_writer.writerow(['repo_name','name','path','sha','file_sha','size','url','url_repos','html_url','git_url', 'download_content_url','type','encoding'])

path_input="../data/merged_split_data_writer_path_WITHOUT_DUPLICATED_SHA_AND_REMOVED_FORKED_PROJECTS.csv"
df=pd.read_csv(path_input)

    
ct=0
counter=0
index = 0

def get_data(url, ct):
    while True:
        data, ct,request_code  = GitHub(url, ct).getResponse()
        if data != None:
            break
        if request_code==404:
            break
        time.sleep(5)
    return data, ct

url_list = df.file_url.values.tolist()
file_sha_list = df.file_sha.values.tolist()
file_path_list = df.file_path.values.tolist()
repo_name_list = df.repo_name.values.tolist()

#we splitted the range of data , to run multiple codes parallely.
for index in range (18000):
    repo_name = repo_name_list[index] #row['repo_name']
    url = url_list[index] #row['file_url']
    file_sha_= file_sha_list[index] #row['file_sha']
    file_path_ = file_path_list[index] #row['file_path']
    counter +=1
    data,ct=get_data(url,ct)
    sha_substr = str(file_sha_)[0:8]
    if not os.path.exists(download_path+sha_substr): 
        Path(download_path+sha_substr).mkdir(parents=True, exist_ok=True)
    if data != None:
        address=data["path"] 
        adress_folder=""
        if '/' in address: 
            adress_folder=data["path"].replace(data["path"].split('/')[-1], '')
        if not os.path.exists(download_path+sha_substr+"/"+adress_folder):
            Path(download_path+sha_substr+"/"+adress_folder).mkdir(parents=True, exist_ok=True)
        if ct==len(get_tokens()): 
            ct=0
        header = {'Authorization': 'token %s' % get_tokens()[ct]}
        r = requests.get(data['download_url'], stream=True, headers=header)
        with open(download_path+sha_substr+"/"+data["path"], 'wb') as f: 
            for chunk in r.iter_content():
                f.write(chunk)
                f.flush()
        row_data = [repo_name,data['name'],data['path'],data['sha'],sha_substr, data['size'],url, data['url'],data['html_url'],data['git_url'],data['download_url'],data['type'],data['encoding']]

        data_content_writer.writerow(row_data) 

        if counter%100==0:
            print(index, counter,  repo_name)
            data_content_file.close()
            data_content_file = open(path_output + 'url_of_download_content_{}.csv'.format(version) , mode='a+', newline='',encoding='utf-8')
            data_content_writer = csv.writer(data_content_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
data_content_file.close()
