# -*- coding: utf-8 -*-
"""Getting_repo_details.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJDB5Jkgi4fiB3iKXAG9DWulaA1LesZ7
"""

import csv
import pandas as pd
import time

import urllib.request
import json

def get_tokens():
    tokens = ["123456789"]#add your tokens here
    return tokens

class GitHub:
    def __init__(self, url, ct):
        self.ct = ct
        self.url = url
    def getResponse(self):
        jsonData = None
        request_code=200
        try:
            if self.ct == len(get_tokens()):
                self.ct = 0
            reqr = urllib.request.Request(self.url)
            reqr.add_header('Authorization', 'token %s' % get_tokens()[self.ct])
            opener = urllib.request.build_opener(urllib.request.HTTPHandler(debuglevel=1))
            request_code=opener.open(reqr).getcode()
            content = opener.open(reqr).read()
            self.ct += 1
            jsonData = json.loads(content)

        except Exception as e:
            if 'HTTP Error 404' in str(e):
              request_code=404
            if 'HTTP Error 401' in str(e):
              request_code=401
            elif not 'HTTP Error 403' in str(e):
                request_code=404


            print("e is:",e)
        return jsonData, self.ct,request_code
    def url_header(self):
        jsonData = None
        try:
            if self.ct == len(get_tokens()):
                self.ct = 0
            reqr = urllib.request.Request(self.url)
            reqr.add_header('Accept', 'application/vnd.github.mercy-preview+json')
            reqr.add_header('Authorization', 'token %s' % get_tokens()[self.ct])
            opener = urllib.request.build_opener(urllib.request.HTTPHandler(debuglevel=1))
            content = opener.open(reqr).read()
            self.ct += 1
            jsonData = json.loads(content)
            return jsonData, self.ct
        except Exception as e:
            print(e)
        return jsonData, self.ct

def get_data(url, ct):
    while True:
        data, ct,request_code  = GitHub(url, ct).getResponse()
        if data != None:
            break
        if request_code==404:
            break
        time.sleep(5)
    return data, ct

class GitHubMeta:

    def __init__(self, repo, ct):
        self.ct = ct
        self.repo = repo



    def repos_contrib_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/contributors?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct =get_data(url2, self.ct)
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
            else:
                break
        return total_contrib, self.ct
    def repos_pr_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/pulls?state=all&page='+str(p)+'&per_page=100&'
            contrib_arrays, self.ct =get_data(url2, self.ct)
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib % 500 == 0:
                    print(' ---- pr: ', total_contrib)
            else:
                break
        return total_contrib, self.ct
    def repos_issues_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/issues?state=all&page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct=get_data(url2, self.ct)
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib%500 == 0:
                    print(' ---- issues: ', total_contrib)
            else:
                break
        return total_contrib, self.ct

    def repos_issues_(self):
        list_of_issue_row=[]
        p = 1
        total_contrib = 0
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/issues?state=all&page='+str(p)+'&per_page=100'
            json_arrays, self.ct =get_data(url2, self.ct)
            p += 1
            if json_arrays != None:
                if len(json_arrays) == 0:
                    break
                total_contrib += len(json_arrays)
                if total_contrib%500 == 0:
                    print(' ---- issues: ', total_contrib)
                for data_obj in json_arrays:
                    issue_number=data_obj['number']
                    issue_title=data_obj['title']
                    issue_created_at = data_obj['created_at']
                    issue_updated_at = data_obj['updated_at']
                    issue_closed_at = data_obj['closed_at']
                    issue_log_in=data_obj['user']['login']
                    label_name=""
                    label_description=""
                    for label in data_obj['labels']:
                        label_name+=label['name']+","
                        if label['description']==None:
                            label_description=" "+","
                        else:
                            label_description+=label['description']+","
                    row=[issue_number,issue_title,issue_created_at,issue_updated_at,issue_closed_at,issue_log_in,label_name,label_description]
                    list_of_issue_row.append(row)
            else:
                break
        return list_of_issue_row, self.ct
    def repos_commits_counts_(self):
        p = 1
        total_contrib = 0
        lastcommit = ''
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/commits?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = get_data(url2, self.ct)

            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if p == 1:
                    if len(contrib_arrays) > 0:
                        lastcommit = contrib_arrays[0]['commit']['author']['date']
                if total_contrib%500 == 0:
                    print(' ---- commits: ', total_contrib)
            else:
                break
            p += 1
        return total_contrib,lastcommit, self.ct
    def repos_releases_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url2 = 'https://api.github.com/repos/'+self.repo+'/releases?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = get_data(url2, self.ct)
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib%500 == 0:
                    print(' ---- releases: ', total_contrib)
            else:
                break
        return total_contrib, self.ct

    def get_repo_details_(self):
        url = 'https://api.github.com/repos/'+self.repo
        json_data, self.ct = get_data(url, self.ct)

        if json_data != None:
            repo_name=json_data['name']
            repo_description=json_data['description']
            repo_forks=json_data['forks']
            repo_size=json_data['size']
            repo_stars=json_data['stargazers_count']
            repo_watch=json_data['watchers_count']
            repo_language=json_data['language']
            repo_created_at=json_data['created_at']
            repo_update_at=json_data['updated_at']
            repo_is_fork_or_not = json_data['fork']
            topics_str = ''
            for topic in json_data['topics']:
                topics_str += str(topic)+', '
            return repo_name,repo_description,repo_forks,repo_size,repo_created_at,repo_update_at,repo_is_fork_or_not, repo_stars, repo_watch, repo_language, topics_str, self.ct
        else:
            return None
list_of_failes_repos=[]
import time
import csv
import pandas as pd
path_input_data='../data/Dec5_merged_name_of_the_repositories.csv'
data=pd.read_csv(path_input_data)
repos_list=data.repo_name.values.tolist()
print('Total repos: ', len(repos_list))
path_output = '../data/'

version = 1
##csv file without the details of the issues
data_file = open(path_output + '/repos_details/repos_details_{}.csv'.format(version), mode='w', newline='',
                 encoding='utf-8')
data_writer = csv.writer(data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
data_writer.writerow(['Repos', 'issues','pr', 'contributors','releases', 'commits','repo_name','repo_description','repo_forks','repo_size','repo_created_at','repo_update_at', 'repo_lastcommit','repo_is_fork_or_not', 'repo_stars', 'repo_watch', 'repo_language', 'topics'])

#csv file of the details of the issues
issues_data_file = open(path_output + '/issues/issues_details_{}.csv'.format(version), mode='w', newline='',encoding='utf-8')
issues_data_writer = csv.writer(issues_data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
issues_data_writer.writerow(['Repos','issue_number','issue_title','issue_created_at','issue_updated_at','issue_closed_at','issue_log_in','label_name','label_description'])

ct = 0
mean_repos = int(len(repos_list)/2)
for i in range(mean_repos):
    print("I is :",i, repos_list[i])
    list_of_issues, ct = GitHubMeta(repos_list[i], ct).repos_issues_()
    issue_count=len(list_of_issues)
    pr, ct = GitHubMeta(repos_list[i], ct).repos_pr_counts_()
    contrib, ct = GitHubMeta(repos_list[i], ct).repos_contrib_counts_()
    release, ct = GitHubMeta(repos_list[i], ct).repos_releases_counts_()
    commits,lastcommit, ct = GitHubMeta(repos_list[i], ct).repos_commits_counts_()

    repo_details = GitHubMeta(repos_list[i], ct).get_repo_details_()

    if repo_details != None:
        repo_name,repo_description,repo_forks,repo_size,repo_created_at,repo_update_at, repo_is_fork_or_not, repo_stars, repo_watch, repo_language, topics_str, ct=repo_details
        data_writer.writerow([repos_list[i], issue_count, pr, contrib, release, commits, repo_name, repo_description,repo_forks, repo_size, repo_created_at, repo_update_at,lastcommit, repo_is_fork_or_not, repo_stars, repo_watch, repo_language, topics_str])
    if i%100==0:
        data_file.close()
        data_file = open(path_output + '/repos_details/repos_details_{}.csv'.format(version), mode='a+', newline='',
                         encoding='utf-8')
        data_writer = csv.writer(data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for row in list_of_issues:
        issues_data_writer.writerow([repos_list[i],row[0], row[1], row[2], row[3], row[4], row[5],row[6], row[7]])

    if i%100==0:

        issues_data_file.close()
        issues_data_file = open(path_output + '/issues/issues_details_{}.csv'.format(version), mode='a+', newline='',encoding='utf-8')
        issues_data_writer = csv.writer(issues_data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

issues_data_file.close()
data_file.close()
