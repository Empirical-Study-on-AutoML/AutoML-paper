# -*- coding: utf-8 -*-
"""December 1st_getting_repo_name_and_data_path.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nhrJNhDUrMVayTrLYJbwsin6awOaFdez
"""

#from google.colab import drive
#drive.mount('/content/drive')

import csv
import pandas as pd
import math
import time
import random
import math
import urllib.request
import json

def get_tokens():
    tokens=["123456789"] #add your tokens here
    return tokens


# 
class GitHub:
    def __init__(self, url, ct):
        self.ct = ct
        self.url = url
    def getResponse(self):
        jsonData = None
        try:
            if self.ct == len(get_tokens()):
                self.ct = 0
            reqr = urllib.request.Request(self.url)
            reqr.add_header('Authorization', 'token %s' % get_tokens()[self.ct])
            opener = urllib.request.build_opener(urllib.request.HTTPHandler(debuglevel=1))
            content = opener.open(reqr).read()
            
            jsonData = json.loads(content)
        except Exception as e:
            print(e, self.url, self.ct)
        self.ct += 1
        return jsonData, self.ct

class GitHubMeta:
    def __init__(self, repo, ct):
        self.ct = ct
        self.repo = repo
    def repos_contrib_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url = 'https://api.github.com/repos/'+self.repo+'/contributors?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = GitHub(url, self.ct).getResponse()
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
            else:
                break
        return total_contrib, self.ct
    def repos_pr_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url = 'https://api.github.com/repos/'+self.repo+'/pulls?state=all&page='+str(p)+'&per_page=100&'
            contrib_arrays, self.ct = GitHub(url, self.ct).getResponse()
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib % 500 == 0:
                    print(' ---- pr: ', total_contrib)
            else:
                break
        return total_contrib, self.ct
    def repos_issues_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url = 'https://api.github.com/repos/'+self.repo+'/issues?state=all&page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = GitHub(url, self.ct).getResponse()
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib%500 == 0:
                    print(' ---- issues: ', total_contrib)
            else:
                break
        return total_contrib, self.ct
    def repos_commits_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url = 'https://api.github.com/repos/'+self.repo+'/commits?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = GitHub(url, self.ct).getResponse()
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib%500 == 0:
                    print(' ---- commits: ', total_contrib)
            else:
                break
        return total_contrib, self.ct
    def repos_releases_counts_(self):
        p = 1
        total_contrib = 0
        while True:
            url = 'https://api.github.com/repos/'+self.repo+'/releases?page='+str(p)+'&per_page=100'
            contrib_arrays, self.ct = GitHub(url, self.ct).getResponse()
            p += 1
            if contrib_arrays != None:
                if len(contrib_arrays) == 0:
                    break
                total_contrib += len(contrib_arrays)
                if total_contrib%500 == 0:
                    print(' ---- releases: ', total_contrib)
            else:
                break
        return total_contrib, self.ct

def get_data(url, ct):
    while True:
        data, ct = GitHub(url, ct).getResponse()
        if data != None:
            break
        time.sleep(5)
    return data, ct
def writting_data_on_csv2(data_writer,total_expected_everything,config,list_of_repos_name_config,url,ct,data_writer_path):
    page = 1
    while True:
        url2 = url+'&page={}&per_page=100'.format(page)
        data, ct = get_data(url2, ct)
        if data != None:
            if len(data['items']) == 0:
                break
            print("         ---- Page {}, total items: {}".format(page, len(data['items'])))
            for obj in data['items']:
                flag = 0
                repo_config_str = str(config)+'; '+str(obj['repository']['full_name'])
                if not repo_config_str in list_of_repos_name_config:
                    list_of_repos_name_config.append(repo_config_str)
                    data_writer.writerow([config,obj['repository']['full_name']])
          
                data_writer_path.writerow([config,obj['repository']['full_name'], obj['name'],obj['path'],obj['sha'],obj['url']])

        print('      ----- We are on page: ', page, ' now!')
        page += 1
        time.sleep(5)
        if page == 11:
            break
    set_repos = set()
    for repo_conf in list_of_repos_name_config:
        set_repos.add(repo_conf.split('; ')[1])
    print( "      ----- Summary total results: {}, unique added repo name with config {}, unique repos: {}".format(total_expected_everything, len(list_of_repos_name_config), len(set_repos)))
    
def writting_data_on_csv(data_writer,total_expected_everything,config,set_of_repos_name,url,ct,data_writer_path):
    page = 1
    while True:
        url = url+'&page={}&per_page=100'.format(page)
        data, ct = get_data(url, ct)
        if data != None:
            if len(data['items']) == 0:
                break
            print("         ---- Page {}, total items: {}".format(page, len(data['items'])))
            for obj in data['items']:
                flag = 0
                set_of_repos_name.add(obj['repository']['full_name'])
                data_writer_path.writerow([config,obj['repository']['full_name'], obj['name'],obj['path'],obj['sha'],obj['url']])

        print('      ----- We are on page: ', page, ' now!')
        page += 1
        time.sleep(5)
        if page == 11:
            break
    print( "      ----- Summary total results: {}, unique added repo name {}".format(total_expected_everything, len(set_of_repos_name)))
    for repo in set_of_repos_name:
        data_writer.writerow([config,repo])

def __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config,config, ct, list_url, size_from=0, size_to = None, expected_size=None, cm_size=0, list_searches_used=[]):
    flag_ = 0
    if size_from != None and size_to == None:
        size = ':>={}'.format(size_from)
        flag_ = 1
    else:
        if str(size_from) + '/' + str(size_to) in list_searches_used:
            size_to += 1
        list_searches_used.append(str(size_from)+'/'+str(size_to))
        size = ':{}..{}'.format(size_from, size_to)
        flag_ = 3
    url = 'https://api.github.com/search/code?q=' + config + '+in:file+size'+size+'+extension:py+extension:ipynb'

    #data, ct = GitHub(url, ct).getResponse()
    data, ct = get_data(url, ct)

    if data != None:
        if data['total_count'] >= 600 and data['total_count'] <= 1000:
            list_url.append(url)
            cm_size += data['total_count']
            print(expected_size, cm_size)
            if cm_size < expected_size:
                if expected_size-cm_size <= 1000:
                    size_from = size_to+1
                    size_to = None
                elif expected_size-cm_size > 1000 and expected_size-cm_size <= 5000:
                    size_from = size_to+1
                    size_to += 50
                elif expected_size - cm_size > 5000:
                    size_from = size_to+1
                    size_to += 20
                __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config, config, ct, list_url=list_url, size_from=size_from, size_to=size_to,
                                            expected_size=expected_size,
                                            cm_size=cm_size,list_searches_used=list_searches_used)


        elif data['total_count'] <= 600 and (cm_size+data['total_count']) < expected_size:
            if size_to == None:
                size_to = size_from + 100
            else:
                if str(size_from) + '/' + str(size_to) in list_searches_used:
                    size_to += random.randint(size_from, size_to)
                else:
                    size_to += math.floor((size_to + size_from) / 2)
            __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config,config, ct, list_url=list_url, size_from=size_from, size_to=size_to,
                                    expected_size=expected_size,
                                    cm_size=cm_size,list_searches_used=list_searches_used)
        elif data['total_count'] <= 600 and (cm_size+data['total_count']) >= expected_size:
            list_url.append(url)
            print(expected_size, cm_size, ' - saving and going out of loop')
        else:
            if flag_ == 1:
                if data['total_count'] >= 2000:
                    size_to = random.randint(size_from, size_from + 200) 

                else:
                    size_to = random.randint(size_from, size_from + 250) 
                __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config, config, ct, list_url=list_url, size_from=size_from, size_to=size_to,
                                        expected_size=expected_size,
                                        cm_size=cm_size,list_searches_used=list_searches_used)
            elif flag_ == 3:
                if str(size_from) + '/' + str(size_to) in list_searches_used:
                    size_to = random.randint(size_from, size_to)
                else:
                    size_to = math.floor((size_to + size_from) / 2)
                __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config, config, ct, list_url=list_url, size_from=size_from, size_to=size_to,
                                        expected_size=expected_size,
                                        cm_size=cm_size,list_searches_used=list_searches_used)
    else:
        time.sleep(5)
        __generate_search_query(data_writer,total_expected_everything,data_writer_path,list_of_repos_name_config, config, ct, list_url=list_url, size_from=size_from, size_to=size_to,
                                expected_size=expected_size,
                                cm_size=cm_size,list_searches_used=list_searches_used)

def main():
    input_data_path='../data/config-files.csv'
    path_output ='../data/'
    version = 11
    data_file = open(path_output + 'Dec1_splited_name_of_the_repositories_part_{}.csv'.format(version), mode='w', newline='',encoding='utf-8')
    data_writer = csv.writer(data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    data_writer.writerow(['config', 'repo_name'])

    data_file_path = open(path_output + 'Dec1_split1_data_writer_path_part_{}.csv'.format(version), mode='w', newline='',encoding='utf-8')
    data_writer_path = csv.writer(data_file_path, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    data_writer_path.writerow(['config','repo_name','file_name','file_path','file_sha','file_url'])

    suggested_configs=[]
    data=pd.read_csv(input_data_path)
    repos_list=data.values.tolist()
    mean_config = int(len(repos_list)/2)
    
    
    readconfig=[]
    for i in [1,2]:
        df_read=pd.read_csv('../data/Dec1_splited_name_of_the_repositories_part_{}.csv'.format(i))
        for conf in df_read.config.tolist():
            readconfig.append(conf)
    for i in range(len(repos_list)):
        for val in repos_list[i]:
            for config_split in str(val.replace("+", "%2B")).replace(' ', '').split(','):
                config = str(config_split.replace("'", "")).replace(' ', '') 
                config = str(config.replace("\n", "")) 
                config = str(config.replace("\r", ""))
                if not config in readconfig: 
                    suggested_configs.append(config_split)

    print('total config: ', len(suggested_configs))


    ct=0
    proj_list = []
    mean_config = int(len(suggested_configs)/2)
    total_expected_everything = 0
    for j in range(mean_config):
        config=suggested_configs[j]

        config = str(config.replace("'", "")).replace(' ', '')
        config = str(config.replace("\n", ""))
        config = str(config.replace("\r", ""))
        if len(config) > 2:
            set_of_repos_name = set()
            list_of_repos_name_config = []

            print("config: ", config)
            url = 'https://api.github.com/search/code?q=' + config + '+in:file+size:>0+extension:py+extension:ipynb'
            count_selected = 0
            list_searches_used = []
            print(url)
            data, ct = get_data(url, ct)
            list_url = []
            if data != None:
                print(config, ' total repos:', data['total_count'])
                total_expected_everything += data['total_count']
                if data['total_count'] <= 1000:
                    writting_data_on_csv2(data_writer,total_expected_everything,config, list_of_repos_name_config,url,ct,data_writer_path)
                else:
                    __generate_search_query(data_writer,total_expected_everything,data_writer_path, list_of_repos_name_config,config, ct, list_url=list_url, size_from=0, size_to=None,
                                            expected_size=data['total_count'], cm_size=0, list_searches_used=list_searches_used)
                    print("Total urls: {}".format(len(list_url)), list_url)
                    for url in list_url:
                        writting_data_on_csv2(data_writer,total_expected_everything,config, list_of_repos_name_config,url,ct,data_writer_path)


    data_file.close()
    data_file_path.close()



if __name__ == '__main__':
    main()
